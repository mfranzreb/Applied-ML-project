{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "num_workers = 8\n",
    "train_dir = os.path.join(os.getcwd(), 'data', 'train')\n",
    "test_dir = os.path.join(os.getcwd(), 'data', 'test')\n",
    "dtype_to_use = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network architecture\n",
    "Layer options (More info at https://pytorch.org/docs/stable/nn.html):\n",
    "+ Linear: fully connected layer\n",
    "+ Conv1d/Conv2d: Convolutional layers\n",
    "+ BatchNorm2d/LayerNorm/InstanceNorm2d: Normalization layers\n",
    "+ Dropout: Dropout layer\n",
    "+ MaxPool2d/AvgPool2d: Pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer1 = nn.Linear(input_size, 20)\n",
    "        self.layer2 = nn.Linear(20, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        print(float(loss))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Assuming each item in the dataset is a tuple of (input, output)\n",
    "        sample = self.data[index]\n",
    "        input_array, output_array = sample[0], sample[1]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_tensor = torch.Tensor(input_array)\n",
    "        output_tensor = torch.Tensor(output_array)\n",
    "\n",
    "        return input_tensor, output_tensor\n",
    "    \n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_ds = None\n",
    "        self.test_ds = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    #Arrays are transposed to make input size 100 instead of 2\n",
    "    def setup(self, stage):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for subdir in os.listdir(train_dir):\n",
    "            if (\"188root\" not in subdir):\n",
    "                continue\n",
    "            input_output = (pd.read_csv(os.path.join(train_dir, subdir, 'polar.csv')).values.transpose(), pd.read_csv(os.path.join(train_dir, subdir, 'coords.csv')).values.transpose())\n",
    "            train_data.append(input_output)\n",
    "            break\n",
    "        for subdir in os.listdir(test_dir):\n",
    "            input_output = (pd.read_csv(os.path.join(test_dir, subdir, 'polar.csv')).values.transpose(), pd.read_csv(os.path.join(test_dir, subdir, 'coords.csv')).values.transpose())\n",
    "            test_data.append(input_output)\n",
    "            break\n",
    "\n",
    "        self.train_ds = CustomDataset(train_data)\n",
    "        self.test_ds = CustomDataset(test_data)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            #num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            #num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceCallback(pl.Callback):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch == trainer.max_epochs - 1:\n",
    "            # Calculate distances for the last epoch\n",
    "            distances = self.calculate_distances(pl_module, self.dataset)\n",
    "            \n",
    "            # Plot average and maximum distances\n",
    "            self.plot_distances(distances)\n",
    "\n",
    "    def calculate_distances(self, pl_module, dataset):\n",
    "        pl_module.eval()\n",
    "        distances = []\n",
    "\n",
    "        for data in DataLoader(dataset, batch_size=1):\n",
    "            inputs, targets = data\n",
    "            with torch.no_grad():\n",
    "                outputs = pl_module(inputs)\n",
    "            distances.append(self.calculate_distance(outputs, targets))\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def calculate_distance(self, output, target):\n",
    "        # Replace this with your distance calculation method\n",
    "        # Here, I'm using L2 distance for illustration purposes\n",
    "        distance = torch.norm(output - target)\n",
    "        return distance.item()\n",
    "\n",
    "    def plot_distances(self, distances):\n",
    "        avg_distance = np.mean(distances)\n",
    "        max_distance = np.max(distances)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(distances, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.axvline(avg_distance, color='red', linestyle='dashed', linewidth=2, label=f'Average Distance: {avg_distance:.2f}')\n",
    "        plt.axvline(max_distance, color='green', linestyle='dashed', linewidth=2, label=f'Max Distance: {max_distance:.2f}')\n",
    "        plt.title('Distance Distribution')\n",
    "        plt.xlabel('Distance')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lightning_model = NN(learning_rate=learning_rate, input_size=input_size, output_size=output_size).to(device)\n",
    "\n",
    "\n",
    "dm = CustomDataModule(batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and plot loss over epochs, as well as average and maximum difference for every sample in the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | layer1  | Linear  | 2.0 K \n",
      "1 | layer2  | Linear  | 2.1 K \n",
      "2 | relu    | ReLU    | 0     \n",
      "3 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] 0.19987015426158905\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 83.56it/s, v_num=3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.19622020423412323\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.19319576025009155\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.19085821509361267\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1889035403728485\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.1871088445186615\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.18538445234298706\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.18367645144462585\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1819576621055603\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.1802108883857727\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.17845088243484497\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.17656253278255463\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.17461460828781128\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.17242315411567688\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.17010506987571716\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1676875650882721\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.16518914699554443\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.16262388229370117\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.1600010246038437\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1573249250650406\n",
      "Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.15459614992141724\n",
      "Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1518140435218811\n",
      "Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.14897885918617249\n",
      "Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.14609314501285553\n",
      "Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1431611180305481\n",
      "Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.14018727838993073\n",
      "Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.13717488944530487\n",
      "Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1341252326965332\n",
      "Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.13103799521923065\n",
      "Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1279125064611435\n",
      "Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.12474927306175232\n",
      "Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.12155100703239441\n",
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.11832321435213089\n",
      "Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.1150737926363945\n",
      "Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.11181215196847916\n",
      "Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.10854803025722504\n",
      "Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.10529061406850815\n",
      "Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.10204795002937317\n",
      "Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.09882684051990509\n",
      "Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.09563294053077698\n",
      "Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.09247121959924698\n",
      "Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.08934614062309265\n",
      "Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.08626191318035126\n",
      "Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.08322255313396454\n",
      "Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.0802319124341011\n",
      "Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.07729362696409225\n",
      "Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.07441101968288422\n",
      "Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.07158699631690979\n",
      "Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.06882397830486298\n",
      "Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.0661238431930542\n",
      "Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.06348797678947449\n",
      "Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.06091732904314995\n",
      "Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.0584125816822052\n",
      "Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.05597420036792755\n",
      "Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.053602468222379684\n",
      "Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.05129734054207802\n",
      "Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.0490584596991539\n",
      "Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.04688506945967674\n",
      "Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.04477614909410477\n",
      "Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.04273059964179993\n",
      "Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.040747519582509995\n",
      "Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.038826342672109604\n",
      "Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.03696686029434204\n",
      "Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.03516899421811104\n",
      "Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.03343259170651436\n",
      "Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.031757231801748276\n",
      "Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.03014216013252735\n",
      "Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.02858635038137436\n",
      "Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.027088673785328865\n",
      "Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.025648022070527077\n",
      "Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.02426333911716938\n",
      "Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.02293362095952034\n",
      "Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.02165783941745758\n",
      "Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.020434893667697906\n",
      "Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.01926363818347454\n",
      "Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.01814289577305317\n",
      "Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.017071517184376717\n",
      "Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.016048390418291092\n",
      "Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.015072419308125973\n",
      "Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.014142459258437157\n",
      "Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.01325730886310339\n",
      "Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.01241566427052021\n",
      "Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.011616148985922337\n",
      "Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.010857367888092995\n",
      "Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]        0.010137932375073433\n",
      "Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.009456443600356579\n",
      "Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.008811602368950844\n",
      "Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.008202161639928818\n",
      "Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.007626901846379042\n",
      "Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.007084595039486885\n",
      "Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.006573998834937811\n",
      "Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.006093847565352917\n",
      "Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.00564287044107914\n",
      "Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.005219811107963324\n",
      "Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.004823437891900539\n",
      "Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.004452545661479235\n",
      "Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.004105947911739349\n",
      "Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.00378247257322073\n",
      "Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]0.00348096527159214\n",
      "Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, v_num=3]         0.0032002979423850775\n",
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 125.10it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:00<00:00, 61.70it/s, v_num=3] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "tb_logger = TensorBoardLogger(\"logs\", name=\"results\")\n",
    "callbacks = [pl.callbacks.LearningRateMonitor(logging_interval='epoch'),\n",
    "             #pl.callbacks.ProgressBar(),\n",
    "             #DistanceCallback(dataset=dm.train_ds),\n",
    "            ]\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, callbacks=callbacks, log_every_n_steps=1, logger=tb_logger)  # Adjust parameters as needed\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, dm)\n",
    "%tensorboard --logdir logs/results/version_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results from testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x95 and 100x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    733\u001b[0m     model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 735\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:778\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[0;32m    775\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    777\u001b[0m )\n\u001b[1;32m--> 778\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    780\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 973\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1009\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-stage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    374\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 375\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    379\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:291\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 291\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    294\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:388\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtest_step_context():\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TestStep)\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[123], line 28\u001b[0m, in \u001b[0;36mNN.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     27\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs, targets)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[1;32mIn[123], line 11\u001b[0m, in \u001b[0;36mNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x95 and 100x20)"
     ]
    }
   ],
   "source": [
    "trainer.test(lightning_model, dm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
